%!TEX root=../Dissertation.tex
\chapter{PRETENDING: THE ROW SPACE OF AN ADJACENCY MATRIX}

At the $21$st British Combinatorial Conference, Peter Cameron posed this question:

\begin{question}[Cameron, \cite{Cam}]\label{question:hood vector} Let $G$ be a nonempty graph and let $A$ be the adjacency matrix of $G$. Is there always a nonzero $\{0,1\}$-vector in $\Row A$ (over $\mathbb{R}$) that is not a row of $A$?
\end{question}

While Cameron posed the question for vectors over the real numbers, $\mathbb{R}$, note that it suffices to work over the rational numbers, $\mathbb{Q}$.

\section{Introduction}

For basic linear algebra terminology, notation, and results, see \cite{Lay}. For basic graph theory, see \cite{West}. Recall the definition of the adjacency matrix of a graph.

\begin{definition}
	\index{adjacency matrix}
	%\index{n-monoplane@$n$-monoplane|see{monoplane}}
	\index{graph!adjacency matrix}
Let $G$ be a graph with vertex set $V\left(G\right) = \left\{v_1, v_2, \dotsc, v_n\right\}$. The \defn{adjacency matrix} of $G$, denoted $\adjm{G}$, is the $n \times n$ $\left\{0,1\right\}$-matrix with $a_{ij} = 1$ if and only if $v_i$ and $v_j$ are adjacent in $G$.
\end{definition}

\begin{center}\hfill
	\begin{tikzpicture}[baseline={(0,0)}]
		\pgfmathsetmacro{\rad}{0.5/sin(pi/3 r)}
		\node[vertex] (u) at (60:\rad){};			\node[vlab] at ($(u)+(45:0.4)$){$u$};
		\node[vertex] (v) at (180:\rad){};			\node[vlab] at ($(v)+(90:0.4)$){$v$};
		\node[vertex] (w) at (300:\rad){};			\node[vlab] at ($(w)+(315:0.4)$){$w$};
		\node[vertex] (x) at ($(v)!1!150:(u)$){};		\node[vlab] at ($(x)+(90:0.4)$){$x$};
		\draw (v) -- (x) (v) -- (w) (v) -- (u);
		\draw (u) -- (w);
	\end{tikzpicture}\hfill
	{$\linespread{1}\selectfont \adjm{G} = 
	\begin{array}{c}
	\begin{array}{r*{4}{x{1.1em}}}
		u	& v	& w	& x
	\end{array}\\
	\left[ \begin{array}{r*{4}{x{1.1em}}}
		0	&1	&1	&0\\
		1	&0	&1	&1\\
		1	&1	&0	&0\\
		0	&1	&0	&0
	\end{array} \right]\\\null
	\end{array}$}\hfill\null
\end{center}

\begin{definition}
	\index{rank!of a graph}
	\index{graph!rank}
	The \defn{rank} of a graph $G$ is the rank of $\adjm{G}$ (over $\mathbb{R}$).
\end{definition}

In their unpublished manuscript \cite{ACK}, Akbari, Cameron, and Khosrovshahi noticed that many graph parameters have bounds related to the rank of the graph.

\begin{proposition}[\relax{\cite[p. 10--13]{ACK}}] Let $G$ be a graph with rank $r$. Each of these graph parameters is bounded by a function of $r$.
	\begin{itemize}
		\item The number of connected components (aside from isolated vertices) is at most $\left\lfloor\frac{r}{2}\right\rfloor$ with equality if and only if at most one component is complete tripartite and the rest are complete bipartite.
		\item The clique number $\omega\left(G\right) \leq r$, with equality if and only if $G$ is a complete $r$-partite graph (possibly with isolated vertices).
		\item The chromatic number $\chi\left(G\right)$ is bounded by some function of $r$, but Raz and Spieker \cite{RazSpi} have shown that $\chi\left(G\right)$ is not bounded by any polynomial function of $r$.
		\item For fixed $k$, the smallest number of factors in an edge partition into complete $k$-partite graphs is bounded by an unspecified function of $r$.% mention Kotlov result giving o( (4/3)^r )?
		\item If $G$ has no isolated vertices, then the domination number $\gamma\left(G\right) \leq r$ with equality if and only if $G$ is a complete bipartite graph $K_{k,l}$ with $k,l \geq 2$.
		\item If $G$ has no isolated vertices, then the total domination number $\gamma_{t}\left(G\right) \leq r$ with equality if and only if each component of $G$ is complete bipartite.
		\item If $G$ is connected, then the diameter $\diam G \leq r$.
		\item The order of the largest composition factor of the group $\Aut\left(G\right)$ which is not an alternating group is bounded by $r$ but not by a polynomial function of $r$.
	\end{itemize}
\end{proposition}

They are able to say more, but first we need some definitions.

\begin{definition}
	\index{twin}
	\index{vertex!twin}
	Let $G$ be a graph and let $u,v\in V\left(G\right)$. If $N\left(u\right) = N\left(v\right)$ then we say that $u$ and $v$ are \defn{twins} in $G$.
\end{definition}

Isolated vertices will induce $\zerovect$ as a row of $\adjm{G}$. Twin vertices will produce two copies of the same row in $\adjm{G}$. Therefore adding or deleting isolated vertices or twin vertices will not affect the rank of $G$.

\begin{definition}
	\index{reduced graph}
	\index{graph!reduced}
	A simple graph $G$ is said to be a \defn{reduced graph} if and only if $G$ has no isolated vertices and $G$ has no twins.
\end{definition}

Akbari, Cameron, and Khosrovshahi prove that there are only finitely many reduced graphs with a given rank. They give a lower bound on $m\left(r\right)$, the largest order of a reduced graph with rank $r$: \[ m\left(r\right) \geq \begin{cases}2^{(r+2)/2}-1	&\text{if $r$ is even}\\ 5\cdot 2^{(r-3)/2}-1	&\text{if $r$ is odd and $r > 1$}\end{cases}\] and conjecture that their bound is actually the correct value. If every graph has a vector as described in \autoref{question:hood vector}, then $m\left(r\right)$ is an increasing function. Thus we will reformulate \autoref{question:hood vector} as a conjecture.

\begin{conjecture}\label{conj:Adjacency Matrix}If $G$ is a reduced graph then $\Row \adjm{G}$ contains a nonzero $\{0,1\}$-vector that is not a row of the matrix.
\end{conjecture}

\begin{definition}[hood vector]
	\index{hood vector}
	Let $M$ be a $\{0,1\}$-matrix and let $\vect{v} \in \Row M$ be nonzero. We say $\vect{v}$ is a \defn{hood vector} if $\vect{v}$ is a $\left\{0,1\right\}$-vector and $\vect{v}$ is not a row of $M$. If $M = \adjm{G}$ for some graph $G$, we also say that $\vect{v}$ is a hood vector of $G$.
\end{definition}

Rephrased in this language, \autoref{conj:Adjacency Matrix} says that every reduced graph has a hood vector. A hood vector of a graph `looks like' the rows of $\adjm{G}$, which correspond to the neighborhoods of vertices of $G$. Thus the hood vector `looks like' a neighborhood in $G$.

\begin{definition}
	\index{shadow neighborhood}
	\index{neighborhood!shadow}
	Let $G$ be a graph with a hood vector $\vect{v}$. The \defn{shadow neighborhood} of $\vect{v}$ in $G$ is the set of vertices whose corresponding position in $\vect{v}$ is equal to $1$.
\end{definition}

\begin{definition}
	\index{neighborhood vector}
	\index{neighborhood!vector}
	Let $v$ be a vertex of a graph $G$. The \defn{neighborhood vector} of $v$, denoted $\nvect{v}$, is the row of $\adjm{G}$ corresponding to $v$.
\end{definition}
%\begin{align*}
%	\vect{N\left(v\right)}		&&\vect{N}\left(v\right)	&&N\vv{\left(v\right)}
%\end{align*}
%Let's preserve some citations \nocite{*}.

\section{Basic Results}

Any graph whose adjacency matrix has full rank will have $\onevect$ as a hood vector. Costello and Vu~\cite{CoVu} have shown that the adjacency matrix of a random graph will almost surely have full rank. This suggests that a probabilistic approach will not yield the result. It may be fruitful to find some properties that any counterexample to \autoref{conj:Adjacency Matrix} must satisfy.

\begin{lemma}\label{lem:Edge on Triangle}In any counterexample $G$ to \autoref{conj:Adjacency Matrix}, every edge of $G$ must lie on a triangle.
\end{lemma}
\begin{proof}Assume $G$ has an edge $uv$ not on a triangle. This implies $N\left(u\right) \cap N\left(v\right) = \varnothing$. Therefore $\nvect{u}+\nvect{v}$ is a $\left\{0,1\right\}$-vector. Any vertex with this vector as its neighborhood would have to be adjacent to both $u$ and $v$, which is impossible. Thus, the vector produced is a hood vector.
\end{proof}

\begin{proposition}\label{prop:Disjoint Neighborhoods are Covered}Let $G$ be a counterexample to \autoref{conj:Adjacency Matrix}. If $u$ and $v$ are vertices of $G$ with disjoint neighborhoods, then there exists a vertex $w$ such that $N\left(w\right) = N\left(u\right) \cup N\left(v\right)$.
\end{proposition}
\begin{proof}The vector $\nvect{u}+\nvect{v}$ is a $\{0,1\}$-vector that is $1$ exactly in the positions corresponding to $N\left(u\right) \cup N\left(v\right)$.\end{proof}

\begin{corollary}\label{cor:diam G leq 4}If $G$ is a counterexample to \autoref{conj:Adjacency Matrix} then $\diam G \leq 4$.
\end{corollary}

\begin{proposition}\label{prop:diam G leq 3}If $G$ is a counterexample to \autoref{conj:Adjacency Matrix} then $\diam G \leq 3$.
\end{proposition}
\begin{proof}\Autoref{cor:diam G leq 4} tells us that $\diam G \leq 4$, so assume $\diam G = 4$ to obtain a contradiction. Let $u$ and $v$ be vertices of $G$ such that $d\left(u,v\right) = 4$. Not only are the neighborhoods of $u$ and $v$ disjoint, we also know that no edges run between these neighborhoods. Let $w\in N\left(v\right)$.

\begin{center}\begin{tikzpicture}%[baseline={(0,0)}]
	\node[vertex] (u) at (-2.4,0){}; \node[vlab] at (-2.4,0.3){$u$};
	\node[vertex] (v) at (2.4,0){}; \node[vlab] at (2.4,0.3){$v$};
	\foreach \y in {0.8,0.5,0.2,-0.2,-0.5,-0.8}{
		\coordinate (l) at (-1.2,\y){};
		\coordinate (r) at (1.2,\y){};
		\draw (u) -- (l) (r) -- (v);
		\draw[dashed] (l) to (r);
	};
	\draw[fill=white] (-1.2,0) ellipse (0.35 and 0.8);
	\draw[fill=white] (1.2,0) ellipse (0.35 and 0.8);
	\node[vertex] (w) at (1.2,0){}; \node[vlab] at ($(w)+(90:0.3)$){$w$};
	\draw (v) -- (w);

	\extendtopbound
\end{tikzpicture}\end{center}

The vector $\nvect{u}+\nvect{w}$ is a $\left\{0,1\right\}$-vector since no edges run between $N\left(u\right)$ and $N\left(v\right)$. This vector is $1$ on $v$, all vertices in $N\left(u\right)$, and possibly on some other vertices of no consequence. If this vector were the neighborhood vector of a vertex, that vertex would have to be in $N\left(v\right)$ and adjacent to vertices in $N\left(u\right)$, which is impossible.
\end{proof}

\begin{lemma}\label{lem:Triangle on K_4 or lollipop}In any counterexample $G$ to \autoref{conj:Adjacency Matrix}, every triangle of $G$ must be contained in a $K_4$ or an induced lolliop $L_{3,1}$\index{lollipop graph@lollipop graph ($L_{3,1}$)}.
\end{lemma}
%\begin{figure}[htb]
\begin{center}
	\index{lollipop graph@lollipop graph ($L_{3,1}$)}
	\begin{tikzpicture}[every path/.style={line width=0.5pt}]
		\foreach \v in {0,...,2}{ \node[vertex] (v\v) at (\v*120:0.6){}; };
		\node[vertex] (v3) at (1.2,0){};
		\draw (v0) -- (v1) -- (v2) -- (v0) -- (v3);
		\node[vlab] at (0,-1.2){Lollipop $\smash{L_{3,1}}$};
		\extendtopbound
	\end{tikzpicture}
	%\caption{The lollipop graph $L_{3,1}$}
\end{center}
%\end{figure}
\begin{proof}To obtain a contradiction assume that $u$, $v$, and $w$ are vertices in $G$ of a triangle not contained in any $K_4$ or $L_{3,1}$. Thus every vertex of $G$ is adjacent to exactly $0$ or $2$ of $u$, $v$, and $w$. Therefore the vector $\frac{1}{2}\nvect{u} + \frac{1}{2}\nvect{v} + \frac{1}{2}\nvect{w}$ is a $\{0,1\}$-vector. This vector describes a neighborhood containing $u$, $v$, and $w$, which implies the vector is a hood vector since no vertex in $G$ is adjacent to all three of $u$, $v$, and $w$.
\end{proof}

We can use \autoref{lem:Triangle on K_4 or lollipop} to obtain a nice result, but first we need some definitions.

\begin{definition}[From {\cite[p. 281]{West}}] Let $G$ be a simple graph.

	\index{odd triangle}\index{triangle!odd}A triangle $T$ is \defn{odd} if and only if some vertex of $G$ is adjacent to an odd number of vertices of $T$. That is, there exists $v\in V\left(G\right)$ such that $|N(v) \cap V(T)|$ is odd.

	\index{even triangle}\index{triangle!even}A triangle $T$ is \defn{even} if and only if every vertex of $G$ is adjacent to an even number of vertices of $T$. That is, for all $v\in V\left(G\right)$ we have $|N(v) \cap V(T)|$ is even.
\end{definition}

Thus \autoref{lem:Triangle on K_4 or lollipop} says that in a counterexample, every triangle must be odd. Cameron stated that \autoref{conj:Adjacency Matrix} is true for line graphs. We can now prove this result using this characterization of line graphs (\cite{vRW}, paraphrased from \cite[p. 281]{West}):

\begin{theorem}[van Rooij and Wilf, 1965]
	\label{thm:vRW line graph}
	\index{line graph}
	The graph $G$ is a line graph if and only if $G$ is claw-free\index{claw graph@claw graph ($K_{1,3}$)} and no induced diamond\index{diamond graph@diamond graph ($K_4-e$)} of $G$ has two odd triangles.
\end{theorem}

\begin{center}\hfill
	\begin{tikzpicture}
		\pgfmathsetmacro{\rad}{2/3}
		\foreach \v in {0,1,2} \node[vertex] (v\v) at (90+120*\v:\rad){};
		\node[vertex] (c) at (0,0){};
		\foreach \v in {0,1,2} \draw (v\v) -- (c);
		\node[vlab] at (0,-5/6){Claw $\smash{K_{1,3}}$};
		\extendtopbound
	\end{tikzpicture}\hfill\begin{tikzpicture}
		\foreach \u in {0,1}\node[vertex] (u\u) at (2*\u,0.5){};
		\foreach \v in {0,1}\node[vertex] (v\v) at (1,\v){};
		\foreach \u in {0,1}\foreach \v in {0,1} \draw (u\u) -- (v\v);
		\draw (v0) -- (v1);
		\node[vlab] at (1,-0.5){Diamond $\smash{K_4-e}$};
		\extendtopbound
	\end{tikzpicture}\hfill\null
\end{center}

\begin{theorem}\label{thm:Line Graphs work}Every line graph has a hood vector.
\end{theorem}
\begin{proof}
	Let $G$ be a line graph. If $G$ has an induced diamond, then by \autoref{thm:vRW line graph} at least one of the triangles must be even and so $G$ must have a hood vector. If $G$ does not contain an induced diamond then $G$ is claw-free. Every edge of $G$ must be on a triangle by \autoref{lem:Edge on Triangle}.

Take a maximal clique $C$ in $G$. Since every edge of $G$ is on a triangle, $C$ must have at least three vertices. If $C=G$ then $\adjm{G}$ has full rank and so $\onevect$ is a hood vector. Otherwise, choose a vertex $v$ of $G$ which is adjacent to some $u\in V\left(C\right)$ but is not itself in $C$. Since $C$ is maximal, there is some $w \in V\left(C\right)$ such that $v$ is not adjacent to $w$. In fact $v$ cannot be adjacent to any vertex in $C$ except $u$, since if there were some $x\in V\left(C\right)$ adjacent to $v$, $\left\{u,x,w,v\right\}$ would induce a diamond.
\begin{center}\begin{tikzpicture}
	\draw[gray] (0,0) circle (1); \node[vlab,gray] at (-1.2,0.5){$C$};
	\node[vertex] (W) at (180:0.2){}; \node[vlab] at ($(W)+(90:0.3)$){$w$};
	\node[vertex] (U) at (30:1){}; \node[vlab] at ($(U)+(90:0.3)$){$u$};
	\node[vertex] (X) at (330:1){}; \node[vlab] at ($(X)+(-30:0.3)$){$x$};
	\node[vertex] (V) at ($(X)!1!-60:(U)$){}; \node[vlab] at ($(V)+(90:0.3)$){$v$};
	\draw (U) edge (V)
		(U) edge (W)
		(U) edge (X)
		(W) edge (X)
		(V) edge (X);
\end{tikzpicture}\end{center}

Thus, $G$ is composed of maximal cliques (each containing at least three vertices) joined at single vertices. Furthermore, since $G$ is claw-free no vertex can be in more than two such cliques. Call this collection of maximal cliques $\mathcal{M}$. We divide into cases based on $\left|\mathcal{M}\right|$.

\begin{case}
	If $\left|\mathcal{M}\right|=1$ then $G$ is a complete graph, a situation previously handled.
\end{case}

\begin{case}
	If $\left|\mathcal{M}\right|=2$ then $G$ is a pair of cliques $K_k$ and $K_l$ joined at a vertex $v$. We can obtain $\onevect$ as a hood vector $G$ using the linear combination

	\[ \onevect = \sum_{\mathclap{x \in K_k\setminus \left\{v\right\}}} a \nvect{x} + b \nvect{v} + \sum_{\mathclap{x \in K_l\setminus \left\{v\right\}}}c \nvect{x} \]
	%\begin{align*}
	%	a\text{~on~}&K_k\setminus \left\{v\right\}
	%	&b\text{~on~}&v
	%	&c\text{~on~}&K_l\setminus \left\{v\right\}
	%\end{align*}
where $a$, $b$, $c$ satisfy
	\begin{align*}
		\left(k-2\right)a + b &= 1
		&\left(k-1\right)a + \left(l-1\right)c &= 1
		&b + \left(l-2\right)c &= 1
	\end{align*}
This linear system has the solution
	\begin{align*}
		a	&= \frac{2-l}{3l+3k-2kl-4}
		&b	&= \frac{l+k-kl}{3l+3k-2kl-4}
		&c	&= \frac{2-k}{3l+3k-2kl-4}
	\end{align*}
	To see that this solution is always valid, assume $3l+3k-2kl-4 = 0$. Then
	\begin{equation}\label{eq:line graph proof} l = \frac{3k-4}{2k-3} = \frac{2k-3+k-1}{2k-3} = 1+\frac{k-1}{2k-3}\end{equation}
	Since every clique in $\mathcal{M}$ has at least three vertices, $k\geq 3$ and
	\[ \left(2k-3\right)-\left(k-1\right) = k-2 \geq 3-2 = 1 > 0 \]
	Therefore $2k-3 > k-1$. But then \autoref{eq:line graph proof} forces $1 < l < 2$, which cannot possibly be a valid value for $l$. Thus the solution to this linear system is always valid.
\end{case}

\begin{case}If $\left|\mathcal{M}\right| \geq 3$, there are two possibilities.
	\begin{subcase}First, suppose $\mathcal{M}$ contains three cliques $X$, $Y$, $Z$ which all pairwise intersect. They do so at distinct vertices $v_{XY}$, $v_{XZ}$, and $v_{YZ}$. Let $z\in Z\setminus\!\left\{v_{XZ}, v_{YZ}\right\}$. This means $z$ is adjacent to $v_{XZ}$ and $v_{YZ}$ but not $v_{XY}$. Therefore $\left\{ v_{XY}, v_{XZ}, v_{YZ} \right\}$ induces a diamond, contradicting our assumptions.
	\end{subcase}
	\begin{center}\begin{tikzpicture}
		\draw[gray] (0,1) circle (1); \node[vlab,gray] at (-0.3,1.4){$X$};
		\draw[gray] (0,-1) circle (1); \node[vlab,gray] at (-0.3,-1.4){$Y$};
		\draw[gray] ($(0,-1)!1!-60:(0,1)$) circle (1); \node[vlab,gray] at (2.4,0){$Z$};
		\node[vertex] (W) at (0,0){}; \node[vlab] at ($(W)+(225:0.4)$){$v_{XY}$};
		\node[vertex] (U) at ($(0,1)+(-30:1)$){}; \node[vlab] at ($(U)+(150:0.4)$){$v_{XZ}$};
		\node[vertex] (X) at ($(0,-1)+(30:1)$){}; \node[vlab] at ($(X)+(240:0.3)$){$v_{YZ}$};
		\node[vertex] (V) at ($(X)!1!-60:(U)$){}; \node[vlab] at ($(V)+(90:0.3)$){$z$};
		\draw (U) edge (V)
			(U) edge (W)
			(U) edge (X)
			(W) edge (X)
			(V) edge (X);
	\end{tikzpicture}\end{center}
	
	\begin{subcase}On the other hand if $\mathcal{M}$ has no triple that all pairwise intersect, then taking $X, Y, Z\in \mathcal{M}$, we can assume without loss of generality that $X$ and $Z$ do not intersect. If there exist $x\in X$ and $z\in Z$ such that $x$ is not in a clique containing an element of $Z$ and $z$ is not in a clique containing an element of $X$, then the $\left\{0,1\right\}$-vector $\nvect{x}+\nvect{z}$ is a hood vector.
		
		If such elements do not exist, then (without loss of generality) every element of $X$ shares a clique with an element of $Z$. Let $x_1, x_2, x_3\in X$. There exist $z_1, z_2, z_3\in Z$ and $Y_1, Y_2, Y_3\in \mathcal{M}$ such that $x_i, z_i\in\nobreak V(Y_i)$. Since $\mathcal{M}$ has no triple that all pairwise intersect, two of $\left\{Y_1, Y_2, Y_3\right\}$ must not intersect; without loss of generality we will assume $Y_1$ and $Y_2$ do not intersect. Then the $\left\{0,1\right\}$-vector $\frac{1}{2}\nvect{x_1} + \frac{1}{2}\nvect{x_2} + \frac{1}{2}\nvect{z_1} + \frac{1}{2}\nvect{z_2}$ is a hood vector.
	\end{subcase}
	\begin{center}\begin{tikzpicture}
		%\draw[help lines] (-2,-2) grid (5,2);
		\draw[gray,rounded corners] (-1,-1.6) -- (0,-1.6) -- (0,2) -- (-2.5,2) -- (-2.5,-1.6) -- cycle;
		\node[vlab,gray] at (-2,1.4){$X$};
		\draw[gray,rounded corners] (4,-1.6) -- (5.5,-1.6) -- (5.5,2) -- (3,2) -- (3,-1.6) -- cycle;
		\node[vlab,gray] at (5,1.4){$Z$};
		
		\draw[gray] (1.5,1.5) ellipse (1.5 and 1.1);
		\node[vlab,gray] at (1.5,2.2){$Y_1$};
		\draw[gray] (1.5,-1) ellipse (1.5 and 1.1);
		\node[vlab,gray] at (1.5,-1.8){$Y_2$};
	
		\node[vertex] (x1) at (0,1.5){};
		\node[vertex] (x2) at (0,-1){};
		\node[vertex] (x3) at (-0.75,-1.3){};
		\node[vertex] (x4) at (-1.5,-1){};
		\node[vertex] (x5) at (-1.5,1.5){};
		\node[vertex] (x6) at (-0.75,1.8){};
		\node[vertex] (x7) at (-1.9,0.8){};
		\node[vertex] (x8) at (-1.9,-0.3){};
		\foreach \u in {1,...,7}{ \foreach \v in {\u,...,8}{\draw (x\u) edge (x\v); }; };
	
		\node[vertex] (z1) at (3,1.5){};
		\node[vertex] (z2) at (3,-1){};
		\node[vertex] (z3) at (3.75,-1.3){};
		\node[vertex] (z4) at (4.5,-1){};
		\node[vertex] (z5) at (4.5,1.5){};
		\node[vertex] (z6) at (3.75,1.8){};
		\node[vertex] (z7) at (4.9,0.8){};
		\node[vertex] (z8) at (4.9,-0.3){};
		\foreach \u in {1,...,7}{ \foreach \v in {\u,...,8}{\draw (z\u) edge (z\v); }; };
		
		\draw (x1) edge (z1);
		\draw (x2) edge (z2);
	
		\node[vertex] (y2) at ($(x1)+(30:1)$){};
		\node[vertex] (y4) at ($(x1)+(-30:1)$){};
		\node[vertex] (y1) at ($(z1)+(150:1)$){};
		\node[vertex] (y5) at ($(z1)+(210:1)$){};
		\foreach \y in {1,2,4,5}{ \draw (x1) edge (y\y) 	(z1) edge (y\y);	};
		\foreach \u/\v in {1/2,1/4,1/5,2/4,2/5,4/5}{\draw (y\u) edge (y\v); };
	
		\node[vertex] (y2) at ($(x2)+(30:1)$){};
		\node[vertex] (y4) at ($(x2)+(-30:1)$){};
		\node[vertex] (y1) at ($(z2)+(150:1)$){};
		\node[vertex] (y5) at ($(z2)+(210:1)$){};
		\foreach \y in {1,2,4,5}{ \draw (x2) edge (y\y) 	(z2) edge (y\y);	};
		\foreach \u/\v in {1/2,1/4,1/5,2/4,2/5,4/5}{\draw (y\u) edge (y\v); };
	
		\draw[->,shorten >= 3pt,shorten <= 6pt] (-0.5,2.25) node[vlab]{$x_1$} -- (x1);
		\draw[->,shorten >= 3pt,shorten <= 6pt] (-0.5,-2) node[vlab]{$x_2$} -- (x2);
		\draw[->,shorten >= 3pt,shorten <= 6pt] (3.5,2.25) node[vlab]{$z_1$} -- (z1);
		\draw[->,shorten >= 3pt,shorten <= 6pt] (3.5,-2) node[vlab]{$z_2$} -- (z2);
		\extendtopbound
	\end{tikzpicture}\end{center}
\end{case}

All the cases are accounted for and thus we have the result.
\end{proof}

One might be tempted to try to build a counterexample to \autoref{conj:Adjacency Matrix} by starting with some base graph and appending vertices corresponding to hood vectors in hopes of extinguishing the list. This next proposition shows that such an approach is unlikely to succeed.

\begin{proposition}Let $G$ be a graph with a hood vector $\vect{x}$. Let \[ \vect{x} = c_1 \nvect{v_1} + \dotsb  + c_k\nvect{v_k} + c_{k+1}\nvect{v_{k+1}} + \dotsb + c_n \nvect{v_n} \] where $v_1$, $\dotsc$, $v_k$ are the vertices in the shadow neighborhood of $\vect{x}$ and $v_{k+1}$, $\dotsc$, $v_n$ are the other vertices of $G$. Let $G^\prime$ be the graph obtained from $G$ by appending the vertex with neighborhood vector $\vect{x}$. If $c_1 + \dotsb + c_k \neq 0$ then $G^\prime$ also contains a hood vector.
\end{proposition}
\begin{proof}
	Append the vertex $v_{n+1}$ with neighborhood $v_1, v_2, \dotsc, v_k$ and let \[ \Gamma = c_1 + c_2 + \dots + c_k \neq 0 \]
	We can obtain a new hood vector $\vect{y} = \left\langle y_1, \dotsc, y_n\right\rangle$ by letting
	\[ \vect{y} = \frac{c_1}{\Gamma}\nvect{v_1} + \dotsb + \frac{c_n}{\Gamma}\nvect{v_n} + \frac{\Gamma - 1}{\Gamma}\nvect{v_{n+1}} \]
	which will produce
		\begin{align*}y_1 = y_2 = \dots = y_k = y_{n+1} &= 1	&y_{k+1} = \dots = y_n &= 0\end{align*}
which cannot be the neighborhood vector of any vertex since no vertex in $G$ had neighborhood $v_1, v_2, \dotsc, v_k$.
\end{proof}

\section{Generalization}

What is the adjacency matrix of a graph? Ultimately, it is just a symmetric $\left\{0,1\right\}$-matrix with $0$ on the main diagonal. If \autoref{conj:Adjacency Matrix} is true, then it seems reasonable to ask whether  each of these conditions is necessary. Perhaps \autoref{conj:Adjacency Matrix} follows as a corollary to a more general statement.

\begin{question}Does every $\left\{0,1\right\}$-matrix have a hood vector?
\end{question}

The answer is \emph{no}, as this example shows.

\begin{example}[$\{0,1\}$-matrix with no hood vectors]\label{ex:mat without hood}The matrix $M$ has no hood vectors.
\[ \linespread{1}\setlength{\jot}{0pt}\renewcommand{\arraystretch}{1}\selectfont M = \begin{bmatrix}
	0&1&1&0&0&1&1\\
	1&1&0&1&1&0&0\\
	1&0&1&0&1&1&1\\
	1&1&0&1&0&1&1\\
	0&1&1&0&1&0&0\\
	1&0&1&1&0&0&1\\
	1&0&1&1&0&1&0
\end{bmatrix}\]
\end{example}
\begin{proof} In reduced row echelon form $M$ is
\[ \linespread{1}\setlength{\jot}{0pt}\renewcommand{\arraystretch}{1.1}\selectfont\begin{bmatrix}
	1&0&0&0&0&0&\frac{7}{2}\\
	0&1&0&0&0&0&\frac{3}{2}\\
	0&0&1&0&0&0&\frac{1}{2}\\
	0&0&0&1&0&0&\ntv 3\\
	0&0&0&0&1&0&\ntv 2\\
	0&0&0&0&0&1&\ntv 1\\
	0&0&0&0&0&0&0\\
\end{bmatrix}\]

	Thus $\rank M = 6$ and $\vect{n} = \langle 7, 3, 1, \ntv 6, \ntv 4, \ntv 2, \ntv 2\rangle \in \Nul M$.
Moreover $\Span{\vect{n}} = \Nul M$, so vectors in $\Nul M$ have the form $k\cdot \vect{n}$ for some $k \in \mathbb{R}$. Suppose $\vect{a} = \left\langle a_i\right\rangle \in \Row M$. Then
	\[
		\vect{a}\cdot \vect{n} = 7a_1 + 3a_2 + a_3 + \ntv6 a_4 + \ntv 4 a_5 + \ntv 2 a_6 + \ntv 2 a_7 = 0\] and so \[ 7a_1 + 3a_2 + a_3 = 6 a_4 + 4 a_5 + 2 a_6 + 2 a_7 \]
If $\vect{a}$ is a non-zero $\left\{0,1\right\}$-vector, then since all the left-hand coefficients are odd and all the right-hand coefficients are even, $\vect{a}$ must have a $1$ in exactly two of the first three positions. Therefore the following solutions are the only possible.
	\begin{align*}
		7 + 3 + 0 &= 6 + 4 + 0 + 0	&\langle 1,1,0,1,1,0,0\rangle &= \vect{r_2}\\
		7 + 3 + 0 &= 6 + 0 + 2 + 2	&\langle 1,1,0,1,0,1,1\rangle &= \vect{r_4}\\
		7 + 0 + 1 &= 6 + 0 + 2 + 0	&\langle 1,0,1,1,0,1,0\rangle &= \vect{r_7}\\
		7 + 0 + 1 &= 6 + 0 + 0 + 2	&\langle 1,0,1,1,0,0,1\rangle &= \vect{r_6}\\
		7 + 0 + 1 &= 0 + 4 + 2 + 2	&\langle 1,0,1,0,1,1,1\rangle &= \vect{r_3}\\
		0 + 3 + 1 &= 0 + 4 + 0 + 0	&\langle 0,1,1,0,1,0,0\rangle &= \vect{r_5}\\
		0 + 3 + 1 &= 0 + 0 + 2 + 2	&\langle 0,1,1,0,0,1,1\rangle &= \vect{r_1}
	\end{align*}
Thus $\vect{a}$ must be a row of $M$, and so $M$ has no hood vectors.
\end{proof}

Note that $M$ is not symmetric and has some $1$s on the main diagonal. Further, $M^T$ has forty hood vectors. Thus it seems unlikely that a simple modification to $M$ will produce a counterexample to \autoref{conj:Adjacency Matrix}.
\newcommand{\bxl}[1]{\hspace{-\arraycolsep}\begin{array}{|c|}\hline #1\\\hline\end{array}\hspace{-\arraycolsep}}
\[ \linespread{1}\selectfont M = \left[
	\begin{array}{ccccccc}
		0		&1		&1		&\bxl{0}	&0		&1		&1\\
		1		&\bxl{1}	&0		&1		&1		&0		&0\\
		1		&0		&\bxl{1}	&0		&1		&1		&1\\
		\bxl{1}	&1		&0		&\bxl{1}	&0		&1		&1\\
		0		&1		&1		&0		&\bxl{1}	&0		&0\\
		1		&0		&1		&1		&0		&0		&1\\
		1		&0		&1		&1		&0		&1		&0
	\end{array} \right]
\]

That said, \autoref{ex:mat without hood} generalizes to an entire family of $\left\{0,1\right\}$-matrices without hood vectors.

\begin{example}Let $m$ be even. Then the matrix $M$ has no hood vectors.
\[	\linespread{1}\selectfont M = 
	\begin{array}{c@{}c}
		\left[
		\begin{array}{ccccc}
			0	& 1	& 1	& 0	& 0\\
			1	& 1	& 0	& 1	& 1\\
			1	& 0	& 1	& 0	& 1\\
			1	& 1	& 0	& 1	& 0\\
			0	& 1	& 1	& 0	& 1\\\hline
%
			1	& 0	& 1	& 1	& 0\\
			1	& 0	& 1	& 1	& 0\\
			&&&\vdots\\
			1	& 0	& 1	& 1	& 0
		\end{array}\right.
		&\overbrace{\begin{array}{cccc}
			1	& 1	& \ldots & 1\\
			0	& 0	& \ldots	& 0\\
			1	& 1	& \ldots	& 1\\
			1	& 1	& \ldots	& 1\\
			0	& 0	& \ldots	& 0\\\hline
%
			0	& 1	& \ldots	& 1\\
			1	& 0	& 		& 1\\
			\vdots&&\ddots\\
			1	& 1	& \ldots	& 0
		\end{array}}^m
		\left.\vphantom{\begin{array}{c}a\\a\\a\\a\\a\\a\\a\\a\end{array}}\right]
	\end{array}
\]
\end{example}
\begin{proof}We will construct a vector $\vect{n} \in \Nul M$ that is orthogonal only to non-zero $\left\{0,1\right\}$-vectors that are rows of $M$. Let $\vect{n} = \left\langle a, b, c, x, y, w, w, \dotsc, w\right\rangle$. Then if $\vect{n} \in \Nul M$,
\begin{align*}
	b + c + mw &= 0		&a + b + x + mw &= 0\\
	a + b + x + y &= 0		&b + c + y &= 0\\
	a + c + y + mw &= 0	&a + c + x + \left(m-1\right) w &= 0
\end{align*}

Thus
\begin{align*}
	y &= mw		&x &= \left(m+1\right) w
\end{align*}

And so
\begin{align*}
	a + b + \left(2m+1\right)w &= 0		&&			&a &= \ntv\left(3m+1\right)w/2\\
	a + c + 2mw &= 0					&\implies	&	&b &= \ntv\left(m+1\right)w/2\\
	b + c + mw &= 0					&&			&c &= \ntv\left(m-1\right)w/2
\end{align*}

Let $w= \ntv 2$. Then $\vect{n} = \left\langle 3m+1, m+1, m-1, \ntv 2m, \ntv 2m-2, \ntv 2, \ntv 2, \dotsc, \ntv 2 \right\rangle$. By construction $\vect{n} \in \Nul M$. On the other hand, if $\vect{v}\cdot \vect{n} = 0$ for some non-zero $\left\{0,1\right\}$-vector $\vect{v}$, then exactly two of the first three entries in $\vect{v}$ must be $1$, and all possible ways to balance such a sum are already present in $M$.
\end{proof}

Each member of this infinite family of examples has one asymmetry and four non-zero values on the main diagonal. However, the transposes of these matrices contain many hood vectors. \Autoref{ex:logarithmic family} will show another family of $\left\{0,1\right\}$-matrices that avoid hood vectors in the matrix as well as the transpose, but that have a non-constant number of asymmetries and non-zero entries on the main diagonal. Furthermore, \autoref{ex:nonbinary logarithmic family} shows that there is a family of symmetric $\left\{0,\frac{1}{2},1\right\}$-matrices without hood vectors.

These examples show that if \autoref{conj:Adjacency Matrix} is true then it is close to the best possible statement. It is hard to imagine that the main diagonal entries are of actual importance, so it is reasonable to strengthen \autoref{conj:Adjacency Matrix} by removing this requirement.

\begin{conjecture}\label{conj:Symmetric Matrix}Every symmetric $\left\{0,1\right\}$-matrix has a hood vector.
\end{conjecture}

\section{Searching and Verification}

This section investigates algorithmic aspects of the problem of finding hood vectors. By expressing the problem of finding a hood vector as an integer linear program we can enlist powerful computer libraries and toolkits to assist us in our search for a counterexample. We also develop a short certificate that allows us to quickly count the number of hood vectors without explicitly listing them. Linear algebra provides the framework for both aspects. Recall the following well-known theorem:

\begin{theorem}\label{thm:Row orth Nul}Given a basis $\vect{n_1}$, $\dotsc$, $\vect{n_k}$ for $\Nul M$, a vector $\vect{x}\in \Row M$ if and only if $\vect{x}\cdot\vect{n_i}$ for all $i\in \left[k\right]$.
\end{theorem}

Thus, if we have a basis for $\Nul M$ we can easily check if a given vector is in $\Row M$. That test will become the linear program. We will approach the certification problem by producing a substitute for the basis, called a monopole\index{monopole}.

\subsection{Linear Programming}

Let $\vect{r_1},\dotsc,\vect{r_n}$ be the rows of the matrix $M$, let $d_i$ be the weight of row $i$, and let $\vect{n_1},\dotsc,\vect{n_k}$ be a basis for $\Nul M$. Suppose we are given a $\left\{0,1\right\}$-vector $\vect{x}$ and we want to check if it is a hood vector of $M$.

To ensure $\vect{x}$ is not already a row of our matrix, we can check that it differs in some position from each $\vect{r_i}$. Since $\vect{x}$ and $\vect{r_i}$ are both $\{0,1\}$-vectors, $\vect{x}\cdot\vect{r_i}$ will count the number of positions in which $\vect{x}$ and $\vect{r_i}$ are both $1$. Similarly $\left(\onevect - \vect{x}\right) \cdot \left(\onevect - \vect{r_i}\right)$ will count the number of positions in which $\vect{x}$ and $\vect{r_i}$ are both $0$. Thus
{\allowdisplaybreaks\begin{align*}
	n &\geq \vect{x}\cdot\vect{r_i} +\left(\onevect - \vect{x}\right) \cdot\left(\onevect - \vect{r_i}\right)\\
	&= \sum_{j=1}^{n}x_j\cdot r_{ij} + \sum_{j=1}^{n}\left(1 - x_j\right)\cdot \left(1 - r_{ij}\right)\\
	&= \sum_{j=1}^{n}x_j\cdot r_{ij} + 1 - x_j - r_{ij} + x_j\cdot r_{ij}\\
	&= \sum_{j=1}^{n}\left(2 x_j\cdot r_{ij} - x_j\right) + \sum_{j=1}^{n}1 - \sum_{j=1}^{n}r_{ij}\\
	&= \sum_{j=1}^{n}\left(2 x_j\cdot r_{ij} - x_j\right) + n - d_i\\
\intertext{and therefore}
	d_i &\geq \sum_{j=1}^{n}x_j\left(2 r_{ij} - 1\right)
\end{align*}}
where equality holds if and only if $\vect{x} = \vect{r_i}$.

With this in mind we can formulate the problem of finding a hood vector as an integer linear program:

\noindent\hfill\parbox{0.5\textwidth}{Maximize:}\hfill\null
\[ \sum_{i=1}^{n} x_i\]
\hfill\parbox{0.5\textwidth}{Subject to:}\hfill\null
\begin{subequations}\begin{align}
	\sum_{j=1}^{n} x_j \cdot n_{ij} 	&=0\ \text{for all}\ i\in \left[k\right]\label{eq:HV linear program 1}\\
		\sum_{j=1}^{n} x_j\left(2r_{ij}-1\right)&\leq d_i -1\ \text{for all}\ i\in \left[n\right]\label{eq:HV linear program 2}\\
		x_i	\in \left\{0,1\right\}&\ \text{for all}\ i\in \left[n\right]\label{eq:HV linear program 3}%0 \leq x_i	&\leq 1
\end{align}\end{subequations}

This program will return a $\left\{0,1\right\}$-vector of maximum weight in $\Row M$ that is not a row of $M$. Thus, if it does not return $\zerovect$ it will return a hood vector, and if it does return $\zerovect$ then $M$ has no hood vectors.

To find the dual problem we need to first replace the equality \altref[constraint]{eq:HV linear program 1} with a pair of inequality constraints.
\begin{align*}
	\sum_{j=1}^{n} x_j \cdot n_{ij}		&\leq 0\ \text{for all}\ i\in \left[k\right]\tag{\ref*{eq:HV linear program 1}$^+$}\label{eq:HV linear program 1+}\\
	-\sum_{j=1}^{n} x_j \cdot n_{ij}	&\leq 0\ \text{for all}\ i\in \left[k\right]\tag{\ref*{eq:HV linear program 1}$^-$}\label{eq:HV linear program 1-}
\end{align*}

We also need to replace the integer linear program with its fractional relaxation by loosening \altref[constraint]{eq:HV linear program 3}:
\begin{align*}
	x_i &\leq 1\ \text{for all}\ i\in \left[n\right]\tag{\ref*{eq:HV linear program 3}$^\prime$}\label{eq:HV linear program 3'}\\
	x_i &\geq 0\ \text{for all}\ i\in \left[n\right]
\end{align*}

We introduce dual variables: $a_1^{+}$, $\dotsc$, $a_k^{+}$ for \altref[constraint]{eq:HV linear program 1+}; $a_1^{-}$, $\dotsc$, $a_k^{-}$ for \altref[constraint]{eq:HV linear program 1-}; $b_1$, $\dotsc$, $b_n$ for \altref[constraint]{eq:HV linear program 2}; and $c_1$, $\dotsc$, $c_n$ for \altref[constraint]{eq:HV linear program 3'}. The dual problem is given by:

\noindent\hfill\parbox{0.75\textwidth}{Minimize:}\hfill\null
\[ \sum_{i=1}^{n} b_i\left(d_i-1\right) + \sum_{i=1}^{n}c_i\]
\hfill\parbox{0.75\textwidth}{Subject to:}\hfill\null
\begin{align*}
	c_j + \sum_{i=1}^{k}n_{ij}\left(a_i^{+}-a_i^{-}\right) + \sum_{i=1}^{n}b_i\left(2r_{ij}-1\right) &\geq 1\ \text{for all}\ j\in \left[n\right]\\
	a_i^{+}, a_i^{-}&\geq 0\ \text{for all}\ i \in \left[k\right]\\
	b_i, c_i &\geq 0\ \text{for all}\ i \in \left[n\right]
\end{align*}
\comments{% a suggestion from TeX.SX
 \begin{alignat*}{2}
    \text{minimize }   & \sum_{i=1}^m c_i x_i + \sum_{j=1}^n d_j t_j\  \\
    \text{subject to } & \sum_{i=1}^m a_{ij} + e_j t_j \geq g_j &,\ & 1\leq j\leq n\\
                       & f_i x_i + \sum_{j=1}^n b_{ij}t_j \geq h_i\ &,\ & 1\leq i\leq m\\
                       & x\geq 0,\ t_j\geq 0\ &,\ & 1\leq j\leq n,\ 1\leq i\leq m
  \end{alignat*}}


\subsection{Monopoles}
\index{monopole|(}

When an $n\times n$ matrix $M$ has rank $n-1$, we can use row reduction to obtain a nice integer vector $\vect{v}$ such that $\Span{\vect{v}} = \Nul M$. Thus we can check if a given $\left\{0,1\right\}$-vector is in $\Row M$ by checking if it is orthogonal to $\vect{v}$ and we can count how many $\left\{0,1\right\}$-vectors are in $\Row M$ by counting how many subcollections of $\left\{v_1, \dotsc, v_n\right\}$ sum to $0$. If $M$ has rank less than $n-1$ it is not immediately obvious that we can find a vector with the same properties as $\vect{v}$ but we will establish that such a vector always exists.

\begin{definition}[monopole]
	\index{monopole}
	We say that $\vect{v}\in\mathbb{Z}^n$ is a \defn{monopole} for a matrix $M$ if for all $\left\{0,1\right\}$-vectors $\vect{x}$,\[ \vect{x}\in \Row M\ \text{if and only if}\ \vect{x}\cdot \vect{v} = 0\]

	We say $\vect{v}$ is a \defn{monopole for a graph} $G$ if $\vect{v}$ is a monopole for $\adjm{G}$. 
\end{definition}

\begin{theorem}\label{thm:monopoles exist}Every $\left\{0,1\right\}$-matrix has a monopole.
\end{theorem}
\begin{proof}Let $M$ be a $\left\{0,1\right\}$-matrix with $n$ columns. If $M$ has full rank then $\zerovect$ is a monopole for $M$. Otherwise, let $\left\{ \vect{n_1}, \dotsc, \vect{n_k} \right\}$ be a basis for $\Nul M$ and assume (without loss of generality) that $\vect{n_i}\in \mathbb{Z}^n$ for all $i \in \left[k\right]$.

Define $m_1$, $\dotsc$, $m_k$ by 
	\begin{align*}
		m_j &= \max_{\vect{x} \in \left\{0,1\right\}^n}\left\{\left|\vect{x}\cdot\vect{n_j}\right|\right\}\\
			&= \max\left\{\left(\text{sum of the positive entries of $\vect{n_j}$}\right),\left|\text{sum of the negative entries of $\vect{n_j}$}\right|\right\}
	\end{align*}
and define $c_1$, $\dotsc$, $c_k$ recursively by
	\begin{align*}
		c_1 &= 1		&c_j &= 1+ \sum_{i=1}^{j-1}c_i\cdot m_i
	\end{align*}
Now let $\vect{v} \in \mathbb{Z}^n$ be defined as \[ \vect{v} = \sum_{i=1}^{k}c_i\cdot \vect{n_i} \]
We will show that $\vect{v}$ is a monopole for $M$. Let $\vect{x} \in \left\{0,1\right\}^n$.

 Assume $\vect{x}\in \Row M$. By \autoref{thm:Row orth Nul} $\vect{x}\cdot\vect{n_j} = 0$ for all $j\in \left[k\right]$.
	\begin{align*}
		\vect{x}\cdot\vect{v} &= \vect{x}\cdot\left(\sum_{i=1}^{k}c_i\cdot \vect{n_i}\right)\\
		&= \sum_{i=1}^{k}c_i \cdot \vect{x}\cdot \vect{n_i}\\
		&= 0
	\end{align*}

Now assume $\vect{x}\cdot\vect{v} = 0$. By \autoref{thm:Row orth Nul} it suffices to show $\vect{x}\cdot\vect{n_j} = 0$ for all $j\in \left[k\right]$. Suppose not. Let $l = \max\left\{i\mid \vect{x}\cdot \vect{n_i} \neq 0\right\}$.
	\begin{align*}
		0 &= \vect{x}\cdot\vect{v} = \vect{x}\cdot \left(\sum_{i=1}^{k}c_i \cdot \vect{n_i}\right)\\
		0 &=  \sum_{i=1}^{l-1}c_i \cdot\vect{x}\cdot\vect{n_i} + c_l\cdot \vect{x}\cdot\vect{n_l} + 0
	\end{align*}
Therefore
	\begin{align*}
		c_l	&\leq \left| c_l\cdot \vect{x}\cdot \vect{n_l}\right|= \left| \sum_{i=1}^{l-1}c_i\cdot \vect{x}\cdot \vect{n_i}\right|\\
			&\leq \sum_{i=1}^{l-1}c_i\cdot \left|\vect{x}\cdot\vect{n_i}\right|\ \text{by the triangle inequality}\\
			&\leq \sum_{i=1}^{l-1}c_i\cdot m_i\\
			&< c_l
	\end{align*}
	a contradiction. Thus $\vect{x}\cdot\vect{n_j} = 0$ for all $j\in \left[k\right]$.
\end{proof}
\index{monopole|)}

\begin{example}\label{ex:first monopole}Neither $M$ nor $M^T$ has any hood vectors.
\[ \linespread{1}\selectfont M = 
	\begin{array}{|@{}c@{}c@{}|@{}c@{}|}\hline
		\begin{array}{cc|}
			1	& 1\\
			1	& 1\\\hline
		\end{array}
		&\begin{array}{cc|c}
			\AJbf1	& \AJbf0	& 1\\
			\AJbf0	& \AJbf1	& 1\\\hline
		\end{array}
		&\begin{array}{cc}
			1	& 0\\
			0	& 1
		\end{array}\\
		\begin{array}{cc|}
			\AJbf0	& \AJbf1\\
			\AJbf1	& \AJbf0\\\hline
			1	& 1
		\end{array}
		&\begin{array}{ccc}
			0	& 0	& 0\\
			0	& 0	& 0\\
			0	& 0	& 0
		\end{array}
		&\begin{array}{cc}
			1	& 0\\
			0	& 1\\\hline
			1	& 1
		\end{array}\\\hline
		\begin{array}{cc}
			1	& 0\\
			0	& 1
		\end{array}
		&\begin{array}{cc|c}
			1	& 0	& 1\\
			0	& 1	& 1
		\end{array}
		&\begin{array}{cc}
			0	& 0\\
			0	& 0
		\end{array}\\\hline
	\end{array}
\]
\end{example}
\begin{proof}In reduced row echelon form $M$ is

\[	\linespread{1}\selectfont% M = 
	\begin{array}{c}
	\begin{array}{r*{6}{x{1.1em}}}
		a	& b	& c	& d	& e	& f		& g
	\end{array}\\
	\left[ \begin{array}{r*{6}{x{1.1em}}}
		1	& 0	& 0	& 0	& 0	& 0		& 1\\
		0	& 1	& 0	& 0	& 0	& 1		& 0\\
		0	& 0	& 1	& 0	& 1	& 0		& \ntv 1\\
		0	& 0	& 0	& 1	& 1	& \ntv 1	& 0\\
		0	& 0	& 0	& 0	& 0	& 0		& 0\\
		0	& 0	& 0	& 0	& 0	& 0		& 0\\
		0	& 0	& 0	& 0	& 0	& 0		& 0
	\end{array} \right]
	\end{array}
\]
\begin{align*}
	a &= \ntv g	&c &= \ntv e + g\\
	b &= \ntv f	&d &= \ntv e + f
\end{align*}
Where $e,f,g$ are independent variables. By choosing
\begin{align*}
	e &= 3^0 = 1		&f &= 3^1 = 3		&g &= 3^2 = 9
\end{align*}
We obtain the monopole
\[\linespread{1}\selectfont
	\begin{array}{r@{}*{7}{c}@{}l}
		& a		& b			& c			& d	& e		& f		& g\\
	\vect{n} = \big\langle%
		&\ntv 9,	&\ntv 3,	&8,	&2,	&1,	&3,	&9%
	&\big\rangle
	\end{array}
\]
\[ \linespread{1}\selectfont M = \left[
	\begin{array}{*{7}{c}}
		1	& 1	& 1	& 0	& 1	& 1	& 0\\
		1	& 1	& 0	& 1	& 1	& 0	& 1\\
		0	& 1	& 0	& 0	& 0	& 1	& 0\\
		1	& 0	& 0	& 0	& 0	& 0	& 1\\
		1	& 1	& 0	& 0	& 0	& 1	& 1\\
		1	& 0	& 1	& 0	& 1	& 0	& 0\\
		0	& 1	& 0	& 1	& 1	& 0	& 0\\
	\end{array} \right]
\]
It is easy to see from the construction that $\vect{n}$ is orthogonal to every row of $M$.  Suppose $\vect{a} = \left\langle a_i\right\rangle$ is a $\left\{0,1\right\}$-vector orthogonal to $\vect{n}$. Then
	\[
		\vect{a}\cdot \vect{n} = \ntv 9a_1 + \ntv 3a_2 + 8a_3 + 2a_4 + 1a_5 + 3a_6 + 9a_7 = 0\]
	and so \begin{equation}\label{eq:logarithmic example 1}%
		8a_3 + 2a_4 + 1a_5 + 3a_6 + 9a_7 = 9a_1 + 3a_2 \end{equation}
	If we can show that \autoref{eq:logarithmic example 1} has exactly seven non-zero solutions then it follows that $M$ has no hood vectors. We do not count the solution where both sides sum to zero, since that represents $\zerovect\cdot \vect{v} = 0$. The right-hand side can sum to $0$, $3$, $9$, or $12$ in exactly one way each.
	\begin{center}\linespread{1}\selectfont\par\null\par\begin{tabular}{r|r}
		\multicolumn{2}{c}{Right}\\\hline
		sum	& ways\\\hline
		0	& 1\\
		3	& 1\\
		9	& 1\\
		12	& 1
	\end{tabular}\\\null\end{center}
	
 We will construct a similar table for the left-hand side by considering each term in sequence. At first, the only possible sum is $0$. After considering $8$, we have one way to get $0$ and one way to get $8$. When considering $x$, to determine the number of ways to get $i$, we just need to consider how many ways we could get $i$ or $i-x$ in the previous step. \Autoref{fig:dynamic sum} illustrates this process. After considering all the terms on the left-hand side we will have a table similar to the one we obtained for the right-hand side.

\newcommand{\drawdynprog}[3]{% \drawdynprog[step][max sum][entry list]
	
	\pgfmathsetmacro{\yskip}{\ys*#1}
	%\foreach \v in {0,...,23}\node[vlab] at (\xs*\v+0.5*\xs,0.75-\yskip){$\v$};
	\draw (0,-\yskip) grid[xstep=\xs,ystep=0.5] (\xs+\xs*#2,1-\yskip);
	\node[vlab] at (-0.5,0.7-\yskip){sum};
	\node[vlab] at (-0.5,0.2-\yskip){ways};
	\foreach \v/\w in {#3}{
		\node[vlab] (s#1_\v) at (\xs*\v+0.5*\xs,0.75-\yskip){$\v$};
		\node[vlab] (w#1_\v) at (\xs*\v+0.5*\xs,0.25-\yskip){$\w$};
	}
}
\begin{figure}[htb]%
\begin{ctikzpicture}
	\pgfmathsetmacro{\xs}{0.5}
	\pgfmathsetmacro{\ys}{3}
	\pgfmathsetmacro{\rmin}{0.15}
	\pgfmathsetmacro{\rmax}{0.75}
	
	\drawdynprog{0}{0}{0/1};

	\drawdynprog{1}{8}{%
		 0/1,  1/0,  2/0,  3/0,  4/0,  5/0,  6/0,  7/0,  8/1%
	};

	\pgfmathsetmacro{\nmax}{1}
	\pgfmathtruncatemacro{\ast}{0}\pgfmathtruncatemacro{\asp}{\ast+1}
	\foreach \u in {0}{
		\pgfmathtruncatemacro{\up}{\u+8}
		\pgfmathsetmacro{\r}{\rmax-0.5*(\rmax-\rmin)}
		\draw[-latex,semithick] ($(w\ast_\u.south)+(0.25*\xs,0)$) %
			|- ($(w\ast_\u.south)+(0.5*\xs,0)+(s\asp_0.north)-(s\asp_0.north)!\r!(w\ast_0.south)$) %
			-| ($(s\asp_\up.north)+(-0.25*\xs,0)$);
		\draw[-latex,semithick] (w\ast_\u.south) -- (s\asp_\u.north);
	};

	\drawdynprog{2}{10}{%
		 0/1,  1/0,  2/1,  3/0,  4/0,  5/0,  6/0,  7/0,  8/1,  9/0, 10/1%
	};

	\pgfmathsetmacro{\nmax}{3}
	\pgfmathtruncatemacro{\ast}{1}\pgfmathtruncatemacro{\asp}{\ast+1}
	\foreach \u in {0,8}{
		\pgfmathtruncatemacro{\up}{\u+2}
		\pgfmathsetmacro{\r}{\rmax-0.5*(\rmax-\rmin)}
		\draw[-latex,semithick] ($(w\ast_\u.south)+(0.25*\xs,0)$) %
			|- ($(w\ast_\u.south)+(0.5*\xs,0)+(s\asp_1.north)-(s\asp_1.north)!\r!(w\ast_1.south)$) %
			-| ($(s\asp_\up.north)+(-0.25*\xs,0)$);
		\draw[-latex,semithick] (w\ast_\u.south) -- (s\asp_\u.north);
	};

	\drawdynprog{3}{11}{%
		 0/1,  1/1,  2/1,  3/1,  4/0,  5/0,  6/0,  7/0,  8/1,  9/1, 10/1, 11/1%
	};

	\pgfmathsetmacro{\nmax}{3}
	\pgfmathtruncatemacro{\ast}{2}\pgfmathtruncatemacro{\asp}{\ast+1}
	\foreach \u in {0,2,8,10}{
		\pgfmathtruncatemacro{\up}{\u+1}
		\pgfmathsetmacro{\r}{\rmax-0.5*(\rmax-\rmin)}
		\draw[-latex,semithick] ($(w\ast_\u.south)+(0.25*\xs,0)$) %
			|- ($(w\ast_\u.south)+(0.5*\xs,0)+(s\asp_1.north)-(s\asp_1.north)!\r!(w\ast_1.south)$) %
			-| ($(s\asp_\up.north)+(-0.25*\xs,0)$);
		\draw[-latex,semithick] (w\ast_\u.south) -- (s\asp_\u.north);
	};

	\drawdynprog{4}{14}{%
		0/1,  1/1,  2/1,  3/2,  4/1,  5/1,  6/1,  7/0,  8/1,  9/1, 10/1, 11/2,%
		12/1, 13/1, 14/1%
	};

	
	\pgfmathsetmacro{\nmax}{4}
	\pgfmathtruncatemacro{\ast}{3}\pgfmathtruncatemacro{\asp}{\ast+1}
	\foreach \u in {0,1,2,3,8,9,10,11}{
		\pgfmathtruncatemacro{\up}{\u+3}
		\pgfmathsetmacro{\r}{\rmax-(\rmax-\rmin)/(\nmax-1)*mod(\u,4)}%{0.94-0.08*\u}
		\draw[-latex,semithick] ($(w\ast_\u.south)+(0.25*\xs,0)$) %
			|- ($(w\ast_\u.south)+(0.5*\xs,0)+(s\asp_1.north)-(s\asp_1.north)!\r!(w\ast_1.south)$) %
			-| ($(s\asp_\up.north)+(-0.25*\xs,0)$);
		\draw[-latex,semithick] (w\ast_\u.south) -- (s\asp_\u.north);
	};

	\drawdynprog{5}{23}{%
		0/1,  1/1,  2/1,  3/2,  4/1,  5/1,  6/1,  7/0,  8/1,  9/2, 10/2, 11/3,%
		12/3, 13/2, 14/2, 15/1, 16/0, 17/1, 18/1, 19/1, 20/2, 21/1, 22/1, 23/1%
	};

	\pgfmathsetmacro{\nmax}{15}
	\pgfmathtruncatemacro{\ast}{4}\pgfmathtruncatemacro{\asp}{\ast+1}
	\foreach \u in {0,1,2,3,4,5,6,8,9,10,11,12,13,14}{
		\pgfmathtruncatemacro{\up}{\u+9}
		\pgfmathsetmacro{\r}{\rmax-(\rmax-\rmin)/(\nmax-1)*\u}
		\draw[-latex,semithick] ($(w\ast_\u.south)+(0.25*\xs,0)$) %
			|- ($(w\ast_\u.south)+(0.5*\xs,0)+(s\asp_1.north)-(s\asp_1.north)!\r!(w\ast_1.south)$) %
			-| ($(s\asp_\up.north)+(-0.25*\xs,0)$);
		\draw[-latex,semithick] (w\ast_\u.south) -- (s\asp_\u.north);
	};

	\node[vlab,anchor=east] at (0,-0.15+\ys*0.5){start};
	\foreach \a/\b in {1/8, 2/2, 3/1, 4/3, 5/9} \node[vlab,anchor=east] at (0,{0.5+\ys*(0.5-\a)}){consider $\b$};
\end{ctikzpicture}
\caption{Building the table of possible sums using dynamic programming}\label{fig:dynamic sum}
\end{figure}

For each possible sum $i$, let $l_i$ be the number of ways to obtain the sum $i$ on the left and $r_i$ be the number of ways to obtain the sum $i$ on the right. The total number of solutions to \autoref{eq:logarithmic example 1} will then be \[ \sum_{i}r_i\cdot l_i \]

\begin{center}\begin{tikzpicture}
	\pgfmathsetmacro{\xs}{0.5}
	\pgfmathsetmacro{\ys}{2}
	\pgfmathsetmacro{\rmin}{0.15}
	\pgfmathsetmacro{\rmax}{0.75}

	\drawdynprog{0}{23}{%
		0/1,  1/1,  2/1,  3/2,  4/1,  5/1,  6/1,  7/0,  8/1,  9/2, 10/2, 11/3,%
		12/3, 13/2, 14/2, 15/1, 16/0, 17/1, 18/1, 19/1, 20/2, 21/1, 22/1, 23/1%
	};
	\node[vlab,anchor=east] at (0,1.2){left};

	\drawdynprog{1}{12}{%
		0/1,  1/0,  2/0,  3/1,  4/0,  5/0,  6/0,  7/0,  8/0,  9/1, 10/0, 11/0, 12/1%
	};

	\foreach \v in {3,9,12}{
		\draw[very thick] (w1_\v.south west) rectangle ($(s0_\v.north)+(0.5*\xs,0)$);
	};
	\node[vlab,anchor=east] at (0,-0.8){right};
\end{tikzpicture}\end{center}
Thus in this case the number of valid solutions is $2\cdot 1 + 2\cdot 1 + 3\cdot 1 = 7$. Therefore $M$ has no hood vectors. The argument for $M^T$ is similar.
\end{proof}

We can describe the general version of the technique we used in \autoref{ex:first monopole} to count vectors orthogonal to the monopole, but first let us establish some simple facts about the structure of monopoles for counterexamples to \autoref{conj:Adjacency Matrix} and \autoref{conj:Symmetric Matrix}.

\begin{lemma}\label{lem:symmetric row wt 1}Let $n \geq 2$ and let $M$ be a $n\times n$ symmetric $\left\{0,1\right\}$-matrix. If $M$ has a row of weight $1$, then $M$ has a hood vector.
\end{lemma}
\begin{proof}
	To obtain a contradiction assume $M$ has no hood vectors. Let $\vect{v}$ be the row of weight $1$. For each row $\vect{u}\neq \vect{v}$, either $\vect{u}+\vect{v}$ or $\vect{u}-\vect{v}$ is also a row of $M$. Thus every column of $M$ must have weight $0$ or at least $2$. But $M$ has a row of weight $1$, contradicting that $M$ is symmetric.
\end{proof}

\begin{proposition}\label{prop:monopole nonzero}
	Let $n \geq 2$ and let $M$ be an $n\times n$ symmetric $\left\{0,1\right\}$-matrix. Let $\vect{v}=\left\langle v_1, \ldots, v_n\right\rangle$ be a monopole for $M$. If $v_i = 0$ for some $i \in \left[n\right]$ then $M$ has a hood vector.
\end{proposition}
\begin{proof}%[Proof of \autoref{prop:monopole nonzero}]
	Without loss of generality we may assume $v_n=0$. Since $\vect{v}\cdot \left\langle 0,0,\ldots, 0,1\right\rangle = 0$, $M$ has a row of weight $1$. Thus $M$ has a hood vector by \autoref{lem:symmetric row wt 1}.
\end{proof}

Therefore the monopoles of any potential counterexample to \autoref{conj:Adjacency Matrix} or \autoref{conj:Symmetric Matrix} cannot have a $0$ in any position.

\begin{proposition}\label{prop:monopole more than one negative}Let $M$ be a $\left\{0,1\right\}$-matrix with no row equal to $\zerovect$ and let $\vect{v}$ be a monopole for $M$. If $\vect{v}$ does not have $0$ in any position and has exactly one negative (or exactly one positive) entry then $M$ has $\onevect$ as a column.
\end{proposition}
\begin{proof}Without loss of generality assume $v_n$ is the only negative entry of $\vect{v}$. Let $\vect{r}$ be a row of $M$. We need to show $r_n = 1$.
	\begin{align}
		r_1\cdot v_1 + \dotsb + r_{n-1}\cdot v_{n-1} + r_n\cdot v_n &= 0\nonumber\\
		r_1\cdot v_1 + \dotsb + r_{n-1}\cdot v_{n-1} &= \ntv r_n\cdot v_n\label{eq:one negative}
	\end{align}
	Since $\vect{r}\neq \zerovect$, both sides of \autoref{eq:one negative} must be nonzero. But the only way the right-hand side is nonzero is if $r_n = 1$. Therefore the $n$th entry of every row of $M$ is a $1$ and so the $n$th column of $M$ is $\onevect$.
\end{proof}

Since $\onevect$ cannot be a column of the adjacency matrix of a graph, any monopole for a counterexample to \autoref{conj:Adjacency Matrix} must have at least two negative (and symmetrically, at least two positive) entries.

Returning to the problem of counting hood vectors, suppose we have a monopole $\vect{v} = \left\langle v_1, \dotsc, v_k, \ntv v_{k+1}, \dotsc, \ntv v_n\right\rangle$. Without loss of generality suppose $v_1 \geq v_2 \geq \dotsm \geq v_k > 0$ and $v_{k+1} \geq v_{k+2} \geq \dotsm \geq v_n > 0$ (recall that \autoref{prop:monopole nonzero} guarantees $v_i \neq 0$). Given $\vect{a} = \left\langle a_i\right\rangle$ is a $\left\{0,1\right\}$-vector orthogonal to $\vect{n}$, we have%
\begin{equation}\label{eq:knapsack}
	v_1 a_1 + \dotsm + v_k a_k = v_{k+1} a_{k+1} + \dotsm + v_n a_n
\end{equation}

Thus the number of solutions to \autoref{eq:knapsack} is equal to the number of ways in which a subcollection of $\left\{ v_1, \dotsc, v_k \right\}$ sums to the same value as a subcollection of $\left\{ v_{k+1}, \dotsc, v_n \right\}$. Phrased this way we have the enumeration version of the \emph{subset sum problem}, a variant of the well-studied \emph{knapsack problem}. This problem admits a pseudo-polynomial time algorithm via dynamic programming---the very algorithm we used in \autoref{ex:first monopole} and illustrated in \autoref{fig:dynamic sum}.

\begin{example}\label{ex:logarithmic}Neither $M$ nor $M^T$ has any hood vectors. The bolded entries are the only asymmetries in $M$.
\[ \linespread{1}\selectfont M = 
	\begin{array}{|@{}c@{}c@{}|@{}c@{}|}\hline
		\begin{array}{cccc|}
			1	&1	&0	&0\\
			1	&1	&0	&0\\
			0	&0	&1	&1\\
			0	&0	&1	&1\\\hline
		\end{array}
		&\begin{array}{cc|ccc}
			\AJbf1	&\AJbf0	&1	&0	&1\\
			\AJbf0	&\AJbf1	&1	&0	&1\\
			\AJbf1	&\AJbf0	&0	&1	&1\\
			\AJbf0	&\AJbf1	&0	&1	&1\\\hline
		\end{array}&\begin{array}{cccc}
			1	&0	&1	&0\\
			0	&1	&0	&1\\
			1	&0	&1	&0\\
			0	&1	&0	&1\\
		\end{array}\\
		\begin{array}{cccc|}
			\AJbf0	&\AJbf1	&\AJbf0	&\AJbf1\\
			\AJbf1	&\AJbf0	&\AJbf1	&\AJbf0\\\hline
			1	&1	&0	&0\\
			0	&0	&1	&1\\
			1	&1	&1	&1\\
		\end{array}&\begin{array}{ccccc}
			0	&0	&0	&0	&0\\
			0	&0	&0	&0	&0\\
			0	&0	&0	&0	&0\\
			0	&0	&0	&0	&0\\
			0	&0	&0	&0	&0\\
		\end{array}&\begin{array}{cccc}
			1	&0	&1	&0\\
			0	&1	&0	&1\\\hline 
			1	&1	&0	&0\\
			0	&0	&1	&1\\
			1	&1	&1	&1\\
		\end{array}\\\hline
		\begin{array}{cccc}
			1	&0	&1	&0\\
			0	&1	&0	&1\\
			1	&0	&1	&0\\
			0	&1	&0	&1\\
		\end{array}&\begin{array}{cc|ccc}
			1	&0	&1	&0	&1\\
			0	&1	&1	&0	&1\\
			1	&0	&0	&1	&1\\
			0	&1	&0	&1	&1\\
		\end{array}&\begin{array}{cccc}
			0	&0	&1	&1\\
			0	&0	&1	&1\\
			1	&1	&0	&0\\
			1	&1	&0	&0\\
		\end{array}\\\hline
		\multicolumn{1}{@{}c@{}}{\begin{array}{cccc}a&b&c&d\end{array}}
		&\multicolumn{1}{@{}c@{}}{\begin{array}{ccccc}e&f&g&h&i\end{array}}
		&\multicolumn{1}{@{}c@{}}{\begin{array}{cccc}j&k&l&m\end{array}}
	\end{array}
\]
\end{example}
\begin{proof}In reduced row echelon form $M$ is

\[	\linespread{1}\selectfont% M = 
	\begin{array}{c}
	\begin{array}{r*{12}{x{1.1em}}}
		a	& b	& c	& d		& e	& f	& g	& h		& i	& j		& k		& l		& m
	\end{array}\\
	\left[ \begin{array}{r*{12}{x{1.1em}}}
		1	& 0	& 0	& \ntv 1	& 0	& 0	& 0	& 0		& 0	& 0		& 1		& \ntv 1	& 0\\
		0	& 1	& 0	& 1		& 0	& 0	& 0	& 0		& 0	& 1		& 0		& 1		& 0\\
		0	& 0	& 1	& 1		& 0	& 0	& 0	& 0		& 0	& 0		& 0		& 1		& 1\\
		0	& 0	& 0	& 0		& 1	& 0	& 0	& 1		& 1	& 1		& 0		& 0		& \ntv 1\\
		0	& 0	& 0	& 0		& 0	& 1	& 0	& 1		& 1	& 0		& 1		& \ntv 1	& 0\\
		0	& 0	& 0	& 0		& 0	& 0	& 1	& \ntv 1	& 0	& \ntv 1	& \ntv 1	& 1		& 1\\
		0	& 0	& 0	& 0		& 0	& 0	& 0	& 0		& 0	& 0		& 0		& 0		& 0\\
		0	& 0	& 0	& 0		& 0	& 0	& 0	& 0		& 0	& 0		& 0		& 0		& 0\\
		0	& 0	& 0	& 0		& 0	& 0	& 0	& 0		& 0	& 0		& 0		& 0		& 0\\
		0	& 0	& 0	& 0		& 0	& 0	& 0	& 0		& 0	& 0		& 0		& 0		& 0\\
		0	& 0	& 0	& 0		& 0	& 0	& 0	& 0		& 0	& 0		& 0		& 0		& 0\\
		0	& 0	& 0	& 0		& 0	& 0	& 0	& 0		& 0	& 0		& 0		& 0		& 0\\
		0	& 0	& 0	& 0		& 0	& 0	& 0	& 0		& 0	& 0		& 0		& 0		& 0
	\end{array} \right]
	\end{array}
\]
This corresponds to 
\begin{align*}
	a	&= \pntv d - k + l		&e	&= \ntv h - i - j + m\\
	b	&= \ntv d - j - l		&f	&= \ntv h - i - k + l\\
	c	&= \ntv d - l - m		&g	&= \pntv h + j + k - l - m
\end{align*}
Where $d, h, i, j, k, l, m$ are independent variables. By choosing
\begin{align*}
	d &= 3^0 = 1		&j &= 3^3 = 27	&m &= 3^6 = 729\\
	h &= 3^1 = 3		&k &= 3^4 = 81\\
	i &= 3^2 = 9		&l &= 3^5 = 243
\end{align*}
We obtain the monopole
\[\linespread{1}\selectfont
	\begin{array}{r@{}*{13}{c}@{}l}
& a		& b			& c			& d	& e		& f		& g			& h	& i	& j		& k	& l			& m\\
	\big\langle%
& 163,	& \ntv 271,	& \ntv 973,	& 1,	& 690,	& 150,	& \ntv 861,	& 3,	& 9,	& 27,	& 81,	& 243,	& 729%
	&\big\rangle
	\end{array}
\]
Verifying that there are exactly thirteen $\left\{0,1\right\}$-vectors orthogonal to this monopole is left as an exercise to the reader.
\end{proof}

The matrices in \autoref{ex:first monopole} and \autoref{ex:logarithmic} both come from a larger family of matrices without hood vectors.

\begin{example}\label{ex:logarithmic family}Neither $M_k$ nor $M_k^T$ has any hood vectors.

	Let $D_{k}$ be the $2k \times 2k$ matrix with $2\times 2$ blocks of $1$s along the main diagonal and $0$ elsewhere. For example,
\[ \linespread{1}\selectfont D_3 =
	\left[\begin{array}{cccccc}
		1	&1	&0	&0	&0	&0\\
		1	&1	&0	&0	&0	&0\\
		0	&0	&1	&1	&0	&0\\
		0	&0	&1	&1	&0	&0\\
		0	&0	&0	&0	&1	&1\\
		0	&0	&0	&0	&1	&1
	\end{array}\right]
\]

	Let $A_k$ be the $2k \times 2^k -1$ matrix whose columns are indicator vectors for a nonempty subset of $\left[k\right]$ with each entry repeated twice. For example,
\[ \linespread{1}\selectfont A_3 =
	\begin{array}{c}
		\begin{array}{@{}*{7}{w{3.5em}@{}}}
			\left\{1\right\}	&\left\{2\right\}	&\left\{3\right\}	&\left\{1,2\right\}	&\left\{1,3\right\}	&\left\{2,3\right\}	&\left\{1,2,3\right\}
		\end{array}\\
		\left[\begin{array}{@{}*{7}{w{3.5em}@{}}}
			1	&0	&0	&1	&1	&0	&1\\
			1	&0	&0	&1	&1	&0	&1\\
			0	&1	&0	&1	&0	&1	&1\\
			0	&1	&0	&1	&0	&1	&1\\
			0	&0	&1	&0	&1	&1	&1\\
			0	&0	&1	&0	&1	&1	&1\\
		\end{array}\right]
	\end{array}
\]

	Let $01_{m\times n}$ be the $m\times n$ matrix whose entries alternate $0$ and $1$ in both row and column and with $0$ in the upper-left entry. Let $10_{m\times n}$ be the same except with $1$ in the upper-left entry. For example,
\[ \linespread{1}\selectfont 01_{3\times 4} = 
	\left[ \begin{array}{cccc}
		0	&1	&0	&1\\
		1	&0	&1	&0\\
		0	&1	&0	&1
	\end{array}\right]
\]

Recall that $J$ is the all-ones matrix. For a given $k$,
\[ M_k = \begin{tikzpicture}[baseline=(c)]
	\foreach \xl/\xv in {a/0, b/2, c/3.25, d/5, e/7}{
		\foreach \yl\yv in {a/0, b/2, c/4, d/5, e/7}{
			\coordinate (\xl\yl) at (\xv,\yv){};
		};
	};
	%\draw[help lines] (aa) grid (ee);
	\draw (aa) rectangle (ee);
	\coordinate (c) at ($(aa)!0.5!(ee)+(0,-6 pt)$){};
	\draw (ad) -- (dd);
	\draw (ac) -- (bc) (dc) -- (ec);
	\draw (ab) -- (eb);
	\draw (bb) -- (be);
	\draw (ca) -- (cb) (cd) -- (ce);
	\draw (da) -- (de);
	

	\node[vlab] at ($(ad)!0.5!(be)$){$D_k$};
	\node[vlab] at ($(bd)!0.5!(ce)$){$10_{2k\times 2}$};
	\node[vlab] at ($(cd)!0.5!(de)$){$A_k$};
	\node[vlab] at ($(dc)!0.5!(ee)$){$10_{2k+2\times 2k}$};
	\node[vlab] at ($(ac)!0.5!(bd)$){$01_{2\times 2k}$};
	\node[vlab] at ($(ab)!0.5!(bc)$){$A_k^T$};
	\node[vlab] at ($(bb)!0.5!(dd)$){\Large $0$};
	\node[vlab] at ($(db)!0.5!(ec)$){$A_k^T$};
	\node[vlab] at ($(aa)!0.5!(cb)$){$10_{2k\times 2k+2}$};
	\node[vlab] at ($(ca)!0.5!(db)$){$A_k$};
	\node[vlab] at ($(da)!0.5!(eb)$){$J-D_k$};
	\end{tikzpicture}
\]
\end{example}

\begin{example}\label{ex:nonbinary logarithmic family}For each member of the family described in \autoref{ex:logarithmic family}, the matrix obtained by replacing the upper-left and lower-right blocks with $\frac{1}{2}J$ and swapping columns $2k+1$ and $2k+2$ yields a symmetric matrix with no hood vectors.
\end{example}

\section{Conclusion}

Thus far we have been unable to transform the families of matrices described in \autoref{ex:logarithmic family} and \autoref{ex:nonbinary logarithmic family} into a counterexample to \autoref{conj:Symmetric Matrix}. However we believe that \autoref{conj:Symmetric Matrix} is false and that some variation of these families should provide the counterexample, and possibly even a counterexample to \autoref{conj:Adjacency Matrix}. If \autoref{conj:Adjacency Matrix} or even \autoref{conj:Symmetric Matrix} is true, \autoref{ex:logarithmic family} and \autoref{ex:nonbinary logarithmic family} demonstrate that it is true by a very narrow margin. Coupled with Costello and Vu's result \cite{CoVu} about random graphs, it is understandable that this problem is so resistant to attack.

Monopoles provide an interesting side channel for attack. Given a monopole, we can count the number of $\left\{0,1\right\}$-vectors orthogonal to the monopole using the dynamic programming approach. In practice this algorithm can test hundreds of millions of monopoles per second on a desktop computer. With enough information about the structure of the monopoles of a potential counterexample we may be able to reduce the search space to a feasible size for a brute-force search. \Autoref{prop:monopole nonzero} and \autoref{prop:monopole more than one negative} are first steps in this direction. If a monopole has very few negative (or symmetrically, very few positive) entries then the search algorithm can be made even more efficient by limiting the growth of the dynamic programming table. \Autoref{prop:monopole more than one negative} showed us that we cannot hope to have just a single negative entry, but will two suffice? Or perhaps three? In general a constant bound is unlikely, but perhaps there is a reasonable bound if we restrict ourselves to graphs of order at most $n$.